---
title: LR知识点
date: 2020-06-13 21:22:13
catalog: true
tags: 
- LR
- 算法
- 面试
categories: tech
mathjax: true
---

## 简述

LR指逻辑回归，虽然名字是“回归”，但通常被认为是一个分类模型，用来解决分类问题。

简单的说就是一个线性回归的值作为logist函数的输入值，模型输出一个0到1之间的小数。

## 逻辑回归和线性回归的区别

- 线性回归的结果是整个实数范围，逻辑回归的结果是 0 到 1 之间的一个数。
- 二者都是线性模型，逻辑回归把线性回归的结果转化成“可能性”或“概率”，让其结果具有说服性。

## 数学公式推导

$x$表示特征向量，$\theta$表示模型的参数，逻辑回归涉及到的公式如下：

- 线性回归: 根据多个变量预测某一个数值即回归到某一个数值。

$$
y = \theta^T x
$$



- 逻辑斯蒂分布：是一个's'型的函数，函数值范围是（0， 1）

$$
sigmoid(x) = \frac{1}{1+e^{\theta^T x}}
$$



- 几率（odds）：事件发生的概率/事件不发生的概率

$$
odds = \frac{事件发生的概率}{事件不发生的概率}
$$

- 对于逻辑回归：对于二分类问题可以通过如下公式计算得到特征向量$x$属于某一类的概率是多少，然后得到对数几率即线性回归的一个方程式。

$$
p(y=1|x;\theta) = \frac{1}{1+e^{\theta^T x}}
$$

$$
p(y=0|x;\theta) = 1-\frac{1}{1+e^{\theta^T x}} = \frac{e^{\theta^T x}}{1+e^{\theta^T x}}
$$

$$
odds = \frac{p(y=1|x;\theta)}{p(y=0|x;\theta)} = \frac{1}{e^{\theta^T x}}
$$

$$
ln(odds) = -\theta^Tx
$$



## 求解方式及其原理

模型建立好之后就是求解模型参数$\theta$的过程，根据机器学习算法的步骤，此时应当是建立一个代价函数，逻辑回归使用的方法是最大化似然函数（似然函数就是认为当前结果出现的概率最大）。

为了使得整体训练集这多个结果出现的概率最大，我们的代价函数则如下所示：
$$
L(\theta) = \prod_i p(y_i=1|x_i;\theta)^{y_i} p(y_i=0|x_i;\theta)^{1-y_i}
$$
求解方式分为两种：解析式和优化求解，前者是确定函数具有全局最优解才能使用的，后者是非凸函数经常使用的方法，比如梯度下降法。对最大似然函数取对数，那么最大似然函数的推导公式如下：
$$
\begin{align}
f(\theta)=log(L(\theta)) &= log(\prod_i p(y_i=1|x_i;\theta)^{y_i} p(y_i=0|x_i;\theta)^{1-y_i}) \\
&= \sum_i (y_ilog(p(y_i=1|x_i;\theta))+(1-y_i)log(p(y_i=0|x_i;\theta)))\\
&= \sum_i (y_ilog(p(y_i=1|x_i;\theta))+(1-y_i)log(1-p(y_i=1|x_i;\theta))) \\
&= \sum_i (y_ilog(p(y_i=1|x_i;\theta))+log(1-p(y_i=1|x_i;\theta))-y_ilog(1-p(y_i=1|x_i;\theta)))\\
&= \sum_i (y_ilog(\frac{p(y_i=1|x_i;\theta)}{1-p(y_i=1|x_i;\theta)}) + log(1-p(y_i=1|x_i;\theta)))\\
&= \sum_i y_ilog(\frac{p(y_i=1|x_i;\theta)}{p(y_i=0|x_i;\theta)}) + \sum_i log(1-p(y_i=1|x_i;\theta))\\
&= -\sum_i y_i\theta^Tx_i + \sum log(p(y_i=0|x_i;\theta))\\
&= -\sum_i y_i\theta^Tx_i + \sum log(\frac{e^{\theta^Tx_i}}{1+e^{\theta^Tx_i}})\\
&= -\sum_i y_i\theta^Tx_i - \sum log(1 + e^{-\theta^Tx_i})

\end{align}
$$
现在我们需要优化的函数就从$L(\theta)$变成了$log(L(\theta))$,然后我们对每个参数进行求导：
$$
\frac{\partial f(\theta)}{\partial\theta} = -\sum_i y_ix_i + \sum_i\frac{1}{1+e^{-\theta^Tx_i}}e^{-\theta^Tx_i}x_i
$$
令该导数为 0，会发现是没有办法满足的，因为$f(\theta)$是一个非凸函数，所以用解析法是肯定没有办法求得最优解的，那么只能用优化求解的方法，最常见的方法是梯度下降法及其变体（随机梯度下降法、小批量梯度下降法）。

- 梯度下降法：每次都把所有的训练集扔到模型中训练，当样本数量或者特征个数很多的时候，梯度下降法就不适用了。
- 随机梯度下降法：每次仅仅训练一条数据，这会导致训练过程十分缓慢，而且容易陷入局解。
- 小批量梯度下降法：每次训练一部分数据，是梯度下降法和随机梯度下降法中的这种方案。

梯度下降的主要过程分两步：

1. 计算梯度
2. 更新梯度

```
while not 收敛：
    梯度 = 计算模型关于参数θ的梯度
    根据学习率计算出的梯度更新参数θ的梯度
```


## 正则化

正则化是为了解决模型过拟合的问题，是结构风险最小化的一种实现方式，通过在经验风险（似然函数）上加上一个正则化项，来惩罚过大的参数用于缓解过拟合现象。

一般使用两种正则化：

- L1正则化：也叫稀疏规则算子，实现了特征的自动选择即选择对结果有影响的主要变量，对于结果没有影响的特征就会自动的将其删除。
- L2正则化：也叫岭回归和权值衰减，和L1正则化不同，L1是直接删除某个不起作用的特征，L2是把该特征在模型中的作用给减小，即L1是等于0而L2是无限接近于0。

简而言之，L1趋向于产生少量的特征，而其他特征都是0；L2会选择更多的特征，这些特征会接近于0。

## LR效果为啥效果会比线性回归好

线性回归和逻辑回归的区别就是在输出层加了一个sigmoid函数。

- 线性回归：在整个实数领域内的敏感度一致。
- 逻辑回归：一种减小预测范围（从整个实数范围所见到了0到1之间），并且敏感度不一致，逻辑曲线在变量为0时十分敏感，在变量远大于0或者远小于0都不敏感。

## 逻辑回归的输入为什么建议使用离散数据？

- 离散特征的增加和减少都很容易，易于模型的快速迭代
- 离散化后的特征对异常数据具有很强的鲁棒性，使得模型更稳定。比如年龄》30 的值是1，如果一个人的年龄数据是300，又没有经过离散化处理，那么这个异常数据会给模型造成很大的困扰。
- 逻辑回归本身是一个线性模型，表达能力有限，离散化相当于非线性处理过程，某些情况下能够提升模型的表达能力，增强拟合程度。
- 特征交叉并组合，本身有M+N个特征变量变为了M*N个变量，进一步引入了非线性，提升了模型性的表达能力。

## 工程上的LR的并行化怎么做？



## 参考资料

- https://blog.csdn.net/zouxy09/article/details/20319673
- https://blog.csdn.net/cyh_24/article/details/50359055
- https://zhuanlan.zhihu.com/p/30301789
- https://zhuanlan.zhihu.com/p/30822203

