---
title: 自问自答：介绍一下神经网络
catalog: true
tags:
- qa
- 算法
categories: qa
mathjax: true
date: 2020-06-14 21:50:20
---

神经网络是当前最流行的机器学习算法，目前用得用的比较多的就是全连接神经网络、卷积神经网络和一些循环神经网络。神经网络既可以用于回归也可以用于分类问题。

神经网络之所以这么成功是因为它能够捕捉数据之间的非线性交互特征关系，得到训练数据更好的特征表示，然后再经过非线性转换从而预测结果。神经网络属于非线性模型的原因是非线性激活函数的引入，所以一个合适的激活函数可以让模型的性能更加优良，目前主要的激活函数有relu及其变体、sigmoid和tanh函数。

神经网络虽然能够取得较好的结果，但它有仍然具有两个缺点：

- 参数数量多，消耗的系统资源多。
- 是黑盒算法，无法给出一个具体的解释为什么模型会有这样的结果，这也会导致模型在优化上具有一定的难度。

神经网络算法在python中一般使用tf和pytorch实现，其中工业界多使用tf，学术界一般使用pytorch实现模型。

想要训练一个好的神经网络模型我认为得具备以下条件：

1. 大量且高质量的训练数据集。
2. 良好的网络结构，包括合适的隐藏层、神经元个数以及激活函数类型等。
3. 良好的超参数，比如恰当的学习率、迭代次数、批大小等。

**神经网络算法效果不好或者不work的原因及其解决方案**

- 编码与算法不一致（代码写错了）。检查代码是否严格按照算法描述执行，即检查代码是否存在Bug。
- 输入数据的特征工程没做好：对数据进行预处理并对数据进行归一化。
- 忘了正则化。正则化不仅仅只有缓解过拟合的作用，它还引入了随机的过程。
- 超参数设置不合理。导致模型无法学习到最优解而一直停留在局部解。
- 网络结构不合理。使用了不合适的激活函数比如sigmoid、tanh这种容易发生梯度消失、爆炸的函数，或者神经网络层数太多，神经元个数不合适。