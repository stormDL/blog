---
title: 为什么L2正则化有效果
catalog: true
tags:
  - 算法
  - 数学
categories: tech
mathjax: true
date: 2020-08-10 11:27:23
---

假设模型如下：
$$
arg \min _{w} = [y-(\sum_i^n w_ix_i+b)]^2 + \lambda\sum_i^n w_i^2
$$
其中$\lambda$ 表示惩罚力度，$\lambda$越大，惩罚力度就越大，即函数越**平滑**。

本文主要记录这几个问题：

> 1. 为什么L2正则化为让模型的参数变小？
> 2. 模型参数变小后有什么好处？

## 为什么L2正则化为让模型的参数变小？

从函数kkt条件来看：
$$
arg \min _{w} = [y-(\sum_i^n w_ix_i+b)]^2 + \lambda\sum_i^n w_i^2
$$
完全是等价下面的这个
$$
min \quad loss(w)\\
s.t.\quad \sum_i^n w_i^2 \leq \eta
$$
上面的这个约束满足kkt条件，因此我们可以通过拉格朗日乘子法得到我们最终加了L2正则化的目标函数。

**从约束条件中可以看出**，所有的参数都被限定在了一定的范围内，因此l2正则化不会让模型参数无限的大，模型的参数必须在这个约束内进行优化。

其实还有很多能够解释为什么L2正则化能够让模型的参数变小，比如从贝叶斯角度、损失函数等值线与L2约束的图、求导的角度等，但上面的这个解释我认为是最合适的，也就是说为什么正则化能够让参数的值变小。

## 模型参数变小后有什么好处？

假设原始的输入是$x_i$，但真实世界中数据往往是有噪音的，我们用$\delta$来表示噪音，那么输入数据就变成了$x_i+\delta$，最终的输出也变成了:
$$
\hat{y} = \sum_i^n w_i\cdot (x_i+\delta)+b
$$
而$\hat{y}$与$y$之间的差距就变成了$\hat{y}-y=\sum_i^n w_i\delta$，此时**如果$w_i$的参数很大的话受到噪音$\delta$的影响就会比较大，如果$w_i$比较小的话，那么相对来说抗噪能力会更强，模型的拟合能力也会增强，受到噪音的影响更小**，那么最终的结果离真实结果的距离就会更近，也就是说模型的参数更加平滑。

