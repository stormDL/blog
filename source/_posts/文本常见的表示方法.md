---
title: 文本常见的表示方法
catalog: true
tags:
  - nlp
  - 算法
  - 特征表示
categories: tech
mathjax: true
date: 2020-08-14 16:30:44
---

## 常见的文本表示方法

- one-hot
- bag of words即bow
- n-gram
- tf-idf

## one-hot表示

one-hot是指在一个向量当中，只有一个位置上的值是1，其他位置上的值都是0，常用于表示某个单词，如:

```python
[0,0,0,1]
[1,0,0,0]
...
```

假设我们文本所使用的是一个大小为**n**个不同大小的词表，那么我们定义一个长度为**n**的向量，在这个向量当中的每个位置都代表着某一个单词，那么one-hot向量就可以用来表示某一个单词，如:`[0,0,0,1]`表示词表长度为4（即总共有4个词），当前的这个向量表示第4个单词，其他三个词的表示如下:

```python
[1,0,0,0],表示第一个单词
[0,1,0,0],表示第二个单词
[0,0,1,0],表示第三个单词
```

那么one-hot表示的是一个单词，**一段文本就应该是多个one-hot组合成的矩阵了**，这个不解释了~

- 优点：简洁明了，计算快速
- 缺点：无法表述词与词之间的语义关系，同时因为词表太大容易造成维数灾难等

## bag of word(bow)

bow用于表述一段文本，通过统计各个词出现的次数来表述一篇文档。

假设我们的词表大小为**n**，那么我们定义一个长度为**n**的向量，同one-hot一样每个位置代表一个词，但不同于one-hot用矩阵表述一段文本，bow使用词语出现次数来表示一段话，如这个例子：

```
1. 词表包含四个词：'我'、'你'、'爱'、'呀'
2. 定义一个长度为4的词表向量，
	[1,0,0,0]表示‘我’,即第一个位置表示‘我’
	[0,1,0,0]表示‘你’,即第一个位置表示‘你’
	[0,0,1,0]表示‘爱’,即第一个位置表示‘爱’
	[0,0,0,1]表示‘呀’,即第一个位置表示‘呀’
3. 文本举例
	“我爱你呀”这段话则表示为:[1,1,1,1]，因为每个词都出现了一次
	"我爱你"（或者“你爱我”）这段话则表示为:[1,1,1,0]，‘呀’没有出现，而“我爱你”这三个字都各自出现了一次
	“我我爱你”这段话则表示为:[2,1,1,0]，‘我’出现了两次，‘爱’和‘你’都各自出现了一次，而‘呀’并没有出现
```

- 优点：这是一个基于统计，能够很快得到文本的表示
- 缺点：忽略了文本字词之间的顺序，容易造成信息的丢失or混淆，如“我爱你”和“你爱我”都是同一种表示方法，但他们之间的意思确实不一样的

## n-gram

n-gram在bow的基础上增加了组合字词，即把某个短语看作是新的词，如“我爱你”这里正常来说是‘我’、‘爱’、‘你’三个词，但是当n-gram中的n=2时，则认为词表是由‘我’、‘爱’、‘你’、‘我爱’、‘爱你’这5歌词组成，即认为‘我爱’和‘爱你’都是新的词，例子如下：

```
1. 假设有三个词：‘我’、‘爱’、‘你’
2. 词表本身大小为3
3. 但我们采取n-gram表示的时候，当n=2时，词表大小则变为了 3+新的词组的个数
   假设新的词组有"我爱"、"爱你"、"你爱"，那么词组大小则变为了6，然后使用与bow类似的统计来表示文本
```

- 优点：考虑了字词之间的顺序
- 缺点：增加了词表大小，容易造成维数灾难

## tf-idf

前面的三种表示都是默认每个字词的重要程度是一样的，而在一段文本当中有的词时关键词而有的词就不是很重要，如‘的’、‘吧’、‘啊’等语气词就不是很重要，因此tf-idf考虑了某个词在文本当中的重要程度，其部分计算公式如下：
$$
tf(t) = \frac{词语t在当前文档中出现的次数}{当前文档中词语的总数}\\
idf(t) = log\frac{总的文档数量}{出现该词语的文档数量}
$$
其中tf表示词频（term frequency）,idf表示逆向文件频率(term count),而tf-idf的计算公式则表示为：
$$
tf-idf(t) = tf(t)*idf(t)
$$

- 优点：考虑了词语在文章之间的重要性
- 缺点：用词频来衡量词在文章中的重要性不够全面（如果‘的’之类的没有预处理的话），没有考虑词与词之间的顺序。